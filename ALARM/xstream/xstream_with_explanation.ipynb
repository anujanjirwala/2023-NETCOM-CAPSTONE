{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6a4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Xstream: adapted to categorical/ mixed-type data\n",
    "Reference: @inproceedings{10.1145/3219819.3220107,\n",
    "           author = {Manzoor, Emaad and Lamba, Hemank and Akoglu, Leman},\n",
    "           title = {XStream: Outlier Detection in Feature-Evolving Data Streams},\n",
    "           year = {2018},\n",
    "           isbn = {9781450355520},\n",
    "           publisher = {Association for Computing Machinery},\n",
    "           url = {https://doi.org/10.1145/3219819.3220107},\n",
    "           series = {KDD '18}\n",
    "           }\n",
    "          \n",
    "           @inproceedings{10.1145/3534678.3539076,\n",
    "            author = {Zhang, Sean and Ursekar, Varun and Akoglu, Leman},\n",
    "            title = {Sparx: Distributed Outlier Detection at Scale},\n",
    "            year = {2022},\n",
    "            isbn = {9781450393850},\n",
    "            publisher = {Association for Computing Machinery},\n",
    "            url = {https://doi.org/10.1145/3534678.3539076},\n",
    "            series = {KDD '22}\n",
    "            }\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import sys\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from numpy import linalg as LA\n",
    "import json\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.datasets as dt\n",
    "import seaborn as sns         \n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import mmh3\n",
    "import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def get_mds(shap_inference):\n",
    "    \"\"\"\n",
    "    Get 2 dim projection\n",
    "    :param shap_inference: shap inference array\n",
    "    :return: dataframe with x and y values of mds\n",
    "    \"\"\"\n",
    "    dist_euclid = euclidean_distances(shap_inference)\n",
    "    mds = MDS(dissimilarity=\"precomputed\", random_state=0)\n",
    "    data_transformed = mds.fit_transform(dist_euclid)\n",
    "    data_MDS = pd.DataFrame(data_transformed, columns=[\"x\", \"y\"])\n",
    "    return data_transformed, data_MDS\n",
    "\n",
    "\n",
    "def _hash_string(k, s):\n",
    "    hash_value = int(mmh3.hash(s, signed=False, seed=k))/(2.0**32-1)   \n",
    "    den = 1/3\n",
    "    if hash_value <= den/2.0:\n",
    "        return 1 #-1 \n",
    "    elif hash_value <= den:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "class StreamhashProjection:\n",
    "\n",
    "    def __init__(self, n_components, density=1/3.0, random_state=None):\n",
    "        self.keys = np.arange(0, n_components, 1)\n",
    "        self.constant = np.sqrt(1./density)/np.sqrt(n_components)\n",
    "        self.density = density\n",
    "        self.n_components = n_components\n",
    "        random.seed(random_state)\n",
    "        self.is_R = False\n",
    "        self.feature_names = None\n",
    "        self.R = None\n",
    "        \n",
    "    def initialize_R(self, X,feature_names):\n",
    "        ndim = X.shape[0]\n",
    "        if feature_names is None:\n",
    "            feature_names = [str(i) for i in range(ndim)]\n",
    "        types = [type(X[i]) == str for i in range(ndim)]\n",
    "        feature_name = []\n",
    "        for i in range(ndim):\n",
    "            if types[i]:\n",
    "                feature_name.append(\"%s%s%s\" %(feature_names[i],'.',X[i]))\n",
    "            else:\n",
    "                feature_name.append(feature_names[i])\n",
    "        feature_names = feature_name        \n",
    "        self.R = np.array([[_hash_string(k, f)\n",
    "                       for f in feature_names]\n",
    "                       for k in self.keys])\n",
    "        for i in range(ndim):\n",
    "            if types[i]:\n",
    "                f = feature_names[i]\n",
    "                self.R[:,i] = np.array([_hash_string(k, f) for k in self.keys])\n",
    "        self.is_R = True\n",
    "        self.feature_names =feature_names \n",
    "        \n",
    "\n",
    "    def fit_transform(self, X, feature_names=None): \n",
    "        if not self.is_R:\n",
    "            self.initialize_R(X,feature_names)\n",
    "            \n",
    "        ndim = X.shape[0] \n",
    "        types = [type(X[i]) == str for i in range(ndim)]\n",
    "        X = [1 if types[i] else X[i] for i in range(ndim)]\n",
    "        Y = np.dot(X, self.R.T)       \n",
    "        return Y\n",
    "\n",
    "    def transform(self, X, feature_names=None):\n",
    "        return self.fit_transform(X, feature_names)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class Chain:\n",
    "\n",
    "    def __init__(self, deltamax, depth=25):\n",
    "        k = len(deltamax)\n",
    "        self.deltamax = deltamax # feature ranges\n",
    "        self.depth = depth\n",
    "        self.fs = [np.random.randint(0, k) for d in range(depth)]\n",
    "        self.cmsketches = [None] * depth\n",
    "        self.shift = np.random.rand(k) * deltamax\n",
    "\n",
    "    def fit(self, X, verbose=False, update=False):\n",
    "        prebins = np.zeros(X.shape, dtype=float)\n",
    "        depthcount = np.zeros(len(self.deltamax), dtype=int)\n",
    "        for depth in range(self.depth):\n",
    "            f = self.fs[depth]\n",
    "            depthcount[f] += 1\n",
    "\n",
    "            if depthcount[f] == 1:\n",
    "                prebins[:,f] = (X[:,f] + self.shift[f])/self.deltamax[f]\n",
    "            else:\n",
    "                prebins[:,f] = 2.0*prebins[:,f] - self.shift[f]/self.deltamax[f]\n",
    "\n",
    "            if update:\n",
    "                cmsketch = self.cmsketches[depth]\n",
    "            else:\n",
    "                cmsketch = {}\n",
    "            for prebin in prebins:\n",
    "                l = tuple(np.floor(prebin).astype(int))\n",
    "                if not l in cmsketch:\n",
    "                    cmsketch[l] = 0\n",
    "                cmsketch[l] += 1\n",
    "            self.cmsketches[depth] = cmsketch\n",
    "        return self\n",
    "\n",
    "    def bincount(self, X):\n",
    "        scores = np.zeros((X.shape[0], self.depth))\n",
    "        prebins = np.zeros(X.shape, dtype=float)\n",
    "        depthcount = np.zeros(len(self.deltamax), dtype=int)\n",
    "        for depth in range(self.depth):\n",
    "            f = self.fs[depth] \n",
    "            depthcount[f] += 1\n",
    "\n",
    "            if depthcount[f] == 1:\n",
    "                prebins[:,f] = (X[:,f] + self.shift[f])/self.deltamax[f]\n",
    "            else:\n",
    "                prebins[:,f] = 2.0*prebins[:,f] - self.shift[f]/self.deltamax[f]\n",
    "\n",
    "            cmsketch = self.cmsketches[depth]\n",
    "            for i, prebin in enumerate(prebins):\n",
    "                l = tuple(np.floor(prebin).astype(int))\n",
    "                if not l in cmsketch:\n",
    "                    scores[i,depth] = 0.0\n",
    "                else:\n",
    "                    scores[i,depth] = cmsketch[l]\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def score(self, X, adjusted=False):\n",
    "        # scale score logarithmically to avoid overflow:\n",
    "        #    score = min_d [ log2(bincount x 2^d) = log2(bincount) + d ]\n",
    "        scores = self.bincount(X)\n",
    "        depths = np.array([d for d in range(1, self.depth+1)])\n",
    "        scores = np.log2(1.0 + scores) + depths # add 1 to avoid log(0)\n",
    "        return np.min(scores, axis=1)\n",
    "    \n",
    "    def score_all_depths(self,X):\n",
    "        scores = self.bincount(X)\n",
    "        depths = np.array([d for d in range(1, self.depth+1)])\n",
    "        scores = -(np.log2(1.0 + scores) + depths)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class Chains:\n",
    "    def __init__(self, k=50, nchains=100, depth=25, seed=42, projection = False):\n",
    "        self.nchains = nchains\n",
    "        self.depth = depth\n",
    "        self.chains = []\n",
    "        self.is_projection = projection\n",
    "        if self.is_projection:\n",
    "            self.projector = StreamhashProjection(n_components=k,\n",
    "                                              density=1/3.0,\n",
    "                                              random_state=seed)\n",
    "    def get_projection(self,X,feature_names=None):\n",
    "        if not self.is_projection:\n",
    "            return X\n",
    "        else:\n",
    "            projected_X = []\n",
    "            for i in range(X.shape[0]):\n",
    "                if type(X) == np.ndarray:\n",
    "                    val = X[i]\n",
    "                else:\n",
    "                    val = X.iloc[i]\n",
    "                projected_X.append(self.projector.fit_transform(val,feature_names))\n",
    "            return np.array(projected_X)\n",
    "    \n",
    "    def fit(self, projected_X):\n",
    "        deltamax = np.ptp(projected_X, axis=0)/2.0\n",
    "        deltamax[deltamax==0] = 1.0\n",
    "        for i in tqdm.tqdm(range(self.nchains), desc='Fitting...'):\n",
    "            c = Chain(deltamax, depth=self.depth)\n",
    "            c.fit(projected_X)\n",
    "            self.chains.append(c)\n",
    "\n",
    "    def score(self, projected_X, adjusted=False):\n",
    "        scores = np.zeros(projected_X.shape[0])\n",
    "        for i in tqdm.tqdm(range(self.nchains), desc='Scoring...'):\n",
    "            chain = self.chains[i]\n",
    "            scores += chain.score(projected_X, adjusted)\n",
    "        scores /= float(self.nchains)\n",
    "        return scores\n",
    "\n",
    "\n",
    "def score_in_chains(cf, X_to_explain):\n",
    "    \"\"\"\n",
    "    For each outlier point in topk datapoints,\n",
    "    we detect the ch_chains that contribute to the outlier points\n",
    "    and take the average as the feature_importance\n",
    "    \n",
    "    If projection is set to true, the output is fed into Expalantion class\n",
    "    to trace back the feature importances in the original feature space\n",
    "    \n",
    "    Input: cf: Xstream chains\n",
    "           X_to_explain: the data points to explain\n",
    "           \n",
    "    Ooiutput: np.ndarray, fimportance\n",
    "    \"\"\"\n",
    "    fused = np.zeros((X_to_explain.shape[0],X_to_explain.shape[1], cf.nchains))\n",
    "    score_in_c = np.zeros(X_to_explain.shape)\n",
    "    #max chain val\n",
    "    allscores = []\n",
    "    for cindex in range(0,cf.nchains):\n",
    "        c = cf.chains[cindex]\n",
    "        score_c_AllDepths = -c.score_all_depths(X_to_explain)\n",
    "        allscores.append(score_c_AllDepths)\n",
    "    \n",
    "    max_split_val = np.max(np.array(allscores))\n",
    "    #calculate the scores on splits of each chains\n",
    "    for cindex in range(0, cf.nchains):\n",
    "        c = cf.chains[cindex]\n",
    "        score_c_AllDepths = -c.score_all_depths(X_to_explain)\n",
    "\n",
    "        ind_min = np.argmin(score_c_AllDepths,axis = 1)\n",
    "        score_c = np.zeros((X_to_explain.shape[0],))\n",
    "        for idx,ind in enumerate(ind_min):\n",
    "            score_c[idx] = score_c_AllDepths[idx,ind]\n",
    "        cSplitFeatures = c.fs\n",
    "        \n",
    "        for idx,ind in enumerate(ind_min):\n",
    "            for split_ind, split_f in enumerate(cSplitFeatures[0:ind+1]):\n",
    "                fused[idx,split_f,cindex] = 1\n",
    "                score_in_c[idx,split_f] = score_in_c[idx,split_f] + (max_split_val - score_c_AllDepths[idx,split_ind])\n",
    "                    \n",
    "    fimportance = np.zeros(X_to_explain.shape)\n",
    "    for f in range(fimportance.shape[1]):\n",
    "        #instead of assigining it to zero, we assign +1 to all values\n",
    "        num_in = np.sum(fused[:,f,:],axis =1) + 1.0\n",
    "        fimportance[:,f] = score_in_c[:,f]/num_in\n",
    "\n",
    "    print(\"get Importance of (projection) features BY SCORE \")\n",
    "    return(fimportance)\n",
    "  \n",
    "\n",
    "class Explanation:\n",
    "    def __init__(self,X,feature_names, explanation_type, projdim):\n",
    "        self.ndim = len(feature_names)\n",
    "        self.X = X\n",
    "        self.projdim = projdim\n",
    "        self.explanation_type = explanation_type\n",
    "        self.feature_names = feature_names\n",
    "        if type(X) == np.ndarray:\n",
    "            self.types = [type(X[0][i]) == str for i in range(self.ndim)]\n",
    "        else:\n",
    "            self.types = [type(X.iloc[0][feat]) == str for feat in feature_names]\n",
    "        #self.R_ohe = self.get_Rohe(X)\n",
    "\n",
    "        \n",
    "    def explain(self,fimportance,X):\n",
    "        explain = []\n",
    "        for i in range(fimportance.shape[0]):\n",
    "            if self.explanation_type == \"average\":\n",
    "                explain.append(self.avg_projection(fimportance[i,:],X[i,:]))\n",
    "            elif self.explanation_type == \"random_walk\":\n",
    "                explain.append(self.random_walk_projection(fimportance[i,:],X[i,:]))\n",
    "            elif self.explanation_type == \"hits\":\n",
    "                explain.append(self.hits_projection(fimportance[i,:],X[i,:]))\n",
    "            elif self.explanation_type == \"signed_hits\":\n",
    "                explain.append(self.signed_hits_projection(fimportance[i,:],X[i,:]))\n",
    "            elif self.explanation_type == \"random_walk2\":\n",
    "                explain.append(self.random_walk_projection2(fimportance[i,:],X[i,:]))\n",
    "        #now explain should equal to topk times total_feature_dimensions\n",
    "        #if we have one-hot-encoding, we should only select the feature index with \"1\".\n",
    "        return explain\n",
    "\n",
    "    def get_Rohe(self,X):\n",
    "        ndim = X.shape[0]\n",
    "        if self.feature_names is None:\n",
    "            self.feature_names = [str(i) for i in range(ndim)]\n",
    "        types = [type(X[i]) == str for i in range(ndim)]\n",
    "        feature_name = []\n",
    "        for i in range(ndim):\n",
    "            if types[i]:\n",
    "                feature_name.append(\"%s%s%s\" %(self.feature_names[i],'.',X[i]))\n",
    "            else:\n",
    "                feature_name.append(self.feature_names[i])\n",
    "        feature_names = feature_name \n",
    "        keys = np.arange(0,self.projdim,1)\n",
    "        ohe_R = np.array([[_hash_string(k, f)\n",
    "                       for f in self.feature_names]\n",
    "                       for k in keys])\n",
    "        for i in range(ndim):\n",
    "            if types[i]:\n",
    "                f = feature_names[i]\n",
    "                ohe_R[:,i] = np.array([_hash_string(k, f) for k in keys])\n",
    "        return ohe_R\n",
    "\n",
    "\n",
    "    def avg_projection(self,fimportance,X):   \n",
    "        #get the ohe_to the X\n",
    "        R_ohe = self.get_Rohe(X)\n",
    "        fimpr = deepcopy(fimportance) # which one to use\n",
    "        totalprojperfeat = np.sum(R_ohe, axis=0)\n",
    "        #print(totalprojperfeat)\n",
    "        #totalfeatureperproj = np.sum(self.R_ohe, axis=1)\n",
    "        scaledR = R_ohe.copy()\n",
    "        for p in range(R_ohe.shape[0]):\n",
    "            scaledR[p,:] = scaledR[p,:] * fimpr[p]\n",
    "        origfimportance = np.sum(scaledR, axis=0) / totalprojperfeat\n",
    "        return origfimportance\n",
    "    \n",
    "    \n",
    "    def random_walk_projection2(self,fimportance,X):\n",
    "        R_ohe = self.get_Rohe(X)\n",
    "        pagerank = PageRank(damping_factor=0.2,solver ='diteration')\n",
    "        adjacency = R_ohe\n",
    "        seeds = fimportance.reshape((R_ohe.shape[0],)) # / sum(fimportance.reshape((15,)))\n",
    "        seeds = seeds-np.min(seeds)\n",
    "        pagerank.fit(adj, seeds_row = seeds,force_bipartite = True)\n",
    "        return pagerank.scores_col_\n",
    "\n",
    "\n",
    "    def random_walk_projection(self,fimportance,X,alpha = 0.5,repeat_times = 10):\n",
    "        # attribution to original features\n",
    "        # 1. BY RANDOM WALKS\n",
    "        R_ohe = self.get_Rohe(X)\n",
    "        dr = np.random.rand(R_ohe.shape[1],1)\n",
    "        pr = np.random.rand(R_ohe.shape[0],1)\n",
    "        totalprojperfeat = np.sum(R_ohe, axis=0)\n",
    "        sumr = dr.sum()+pr.sum()\n",
    "        dr = dr / sumr\n",
    "        pr = pr / sumr\n",
    "        \n",
    "        fimpr = fimportance.copy()\n",
    "        #Option 1: normalize the features\n",
    "        \n",
    "        newR = R_ohe.copy()\n",
    "        for p in range(R_ohe.shape[0]):\n",
    "            newR[p,:] = newR[p,:] * fimpr[p]\n",
    "            \n",
    "        origfimportance = np.sum(newR, axis=0) / totalprojperfeat\n",
    "        origfimportance = np.nan_to_num(origfimportance)\n",
    "        \n",
    "        if np.min(fimpr) < 0:\n",
    "            fimpr = fimpr + np.abs(np.min(fimpr))  \n",
    "        fimpr = fimpr / sum(fimpr) # normalize so sum to 1, a prob dist.n\n",
    "\n",
    "        #print(fimpr)\n",
    "        fimpr = fimpr.reshape(newR.shape[0],1)\n",
    "        sums = newR.sum(axis=0,keepdims=1)\n",
    "        sums[sums==0] = 1\n",
    "        A = newR.copy() / sums\n",
    "        B = newR.copy().T\n",
    "        sums = B.sum(axis=0,keepdims=1)\n",
    "        sums[sums==0] = 1\n",
    "        B = B / sums\n",
    "        for i in range(repeat_times):\n",
    "            #print(i)\n",
    "            drnew = alpha * np.dot(B, pr)\n",
    "            #print(drnew.shape)\n",
    "            prnew = alpha * np.dot(A, dr) + (1-alpha) *fimpr \n",
    "            #print(prnew.shape)\n",
    "            denom = (drnew.sum()+prnew.sum())\n",
    "            dr = drnew.reshape(R_ohe.shape[1],1) / denom\n",
    "            pr = prnew.reshape(R_ohe.shape[0],1) / denom\n",
    "        ret_dr = dr/dr.sum()\n",
    "        ret_dr = ret_dr.reshape((dr.shape[0],))\n",
    "        return ret_dr\n",
    "\n",
    "    def hits_projection(self,fimportance,X,alpha=0.5,repeat_time = 20):\n",
    "        R_ohe = self.get_Rohe(X)\n",
    "        newR = R_ohe.copy()\n",
    "        fimpr = fimportance.copy() \n",
    "        # Shift so that minimum is zero\n",
    "        if np.min(fimpr) < 0:\n",
    "            fimpr = fimpr + np.abs(np.min(fimpr))\n",
    "\n",
    "        fimpr = fimpr / LA.norm(fimpr,2)\n",
    "        fimpr = fimpr.reshape(R_ohe.shape[0],1)\n",
    "        pr = fimpr.copy()\n",
    "        B = newR.T\n",
    "        sums = B.sum(axis=0,keepdims=1)\n",
    "        sums[sums==0] = 1\n",
    "        B = B / sums\n",
    "        for i in range(repeat_time):\n",
    "            dr = np.dot(B, pr)\n",
    "            dr = dr.reshape(newR.shape[1],1) / LA.norm(dr,2)\n",
    "            pr = np.dot(newR, dr)\n",
    "            pr = alpha * pr + (1-alpha) * fimpr\n",
    "            pr = pr.reshape(newR.shape[0],1) / LA.norm(pr,2)   \n",
    "        return dr\n",
    "\n",
    "    def signed_hits_projection(self,fimportance,X, epsilon = 0.1, repeat_time = 20):\n",
    "        R_ohe = self.get_Rohe(X)\n",
    "        dr = np.ones(R_ohe.shape[1]) * epsilon # importance\n",
    "        pr = np.ones(R_ohe.shape[0]) * epsilon # authority/accurateness\n",
    "        B = R_ohe.copy().T\n",
    "        sums = B.sum(axis=0,keepdims=1)\n",
    "        sums[sums==0] = 1\n",
    "        scaledR = R_ohe.copy()\n",
    "        scaledRtrans = scaledR.T / sums\n",
    "        for i in range(repeat_time):\n",
    "            dr = np.dot(scaledRtrans, pr)\n",
    "            dr = dr.reshape(scaledR.shape[1],1) / LA.norm(dr,2) # abs(dr.sum())\n",
    "            pr = np.dot(scaledRtrans.T, dr)\n",
    "            pr = pr.reshape(scaledR.shape[0],1) / LA.norm(pr,2) # abs(pr.sum())\n",
    "        return dr\n",
    "\n",
    "\n",
    "class Parameters():\n",
    "    def __init__(self, \n",
    "                 json_file_name,\n",
    "                 ):\n",
    "        with open(json_file_name, 'r') as openfile:\n",
    "            json_ = json.load(openfile)\n",
    "        self.projection = json_[\"projection\"]\n",
    "        self.input_file = json_[\"input_file\"]\n",
    "        self.projdim = json_[\"projdim\"]\n",
    "        self.nchains = json_[\"nchains\"]\n",
    "        self.depth = json_[\"depth\"]\n",
    "        self.output_path = json_[\"output_path\"]\n",
    "        self.dataset_name = json_[\"dataset_name\"]\n",
    "        self.explain = json_[\"explain\"]\n",
    "        self.explain_method = json_[\"explain_method\"]\n",
    "        self.topk = json_[\"topk\"]\n",
    "        self.cluster_num = json_[\"cluster_num\"]\n",
    "        self.has_label = json_[\"has_label\"]\n",
    "        self.use_label = json_[\"use_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131b2227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting...: 100%|█████████████████████████████| 300/300 [04:48<00:00,  1.04it/s]\n",
      "Scoring...: 100%|█████████████████████████████| 300/300 [03:38<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xstream: AP = 0.08006437277041867 AUC = 0.4621689066049845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../parameters.json\"\n",
    "parameters = Parameters(file_path)\n",
    "projdim = parameters.projdim\n",
    "nchains = parameters.nchains\n",
    "depth = parameters.depth\n",
    "projection = parameters.projection\n",
    "input_file = parameters.input_file\n",
    "explain_type = parameters.explain_method\n",
    "output_path = parameters.output_path\n",
    "is_explain = parameters.explain\n",
    "explain_method = parameters.explain_method\n",
    "has_label = parameters.has_label\n",
    "use_label = parameters.use_label\n",
    "#create output directory\n",
    "isExist = os.path.exists(output_path)\n",
    "if not isExist:\n",
    "    os.makedirs(output_path)  \n",
    "\n",
    "#load the data \n",
    "data = pd.read_csv(input_file,delimiter=\",\",index_col='index')\n",
    "\n",
    "list_cat_name = []\n",
    "list_cat = []\n",
    "list_flt = []\n",
    "for i in range(len(data.dtypes)):\n",
    "    if data.dtypes[i] == int or data.dtypes[i] == float:\n",
    "        list_flt.append(i)\n",
    "    else:\n",
    "        list_cat.append(i)\n",
    "        list_cat_name.append(data.columns[i])\n",
    "\n",
    "if has_label:\n",
    "    feature_names = list(data.columns)[0:-1]\n",
    "    label_name = list(data.columns)[-1]\n",
    "    Y = data[label_name]\n",
    "    X = data[feature_names]\n",
    "else:\n",
    "    feature_names = list(data.columns)\n",
    "    X = data[feature_names]\n",
    "\n",
    "\n",
    "topk = parameters.topk\n",
    "if has_label == True and use_label == True:\n",
    "    anomaly_index = [idx for idx,i in enumerate(Y) if i == 1]\n",
    "    topk = len(anomaly_index)\n",
    "\n",
    "val = X.to_numpy()\n",
    "\n",
    "cf = Chains(k=projdim, nchains=nchains, depth=depth, projection = projection)\n",
    "projected_X = cf.get_projection(val,feature_names)\n",
    "cf.fit(projected_X)\n",
    "anomalyscores = -cf.score(projected_X)\n",
    "if has_label:\n",
    "    ap = average_precision_score(Y, anomalyscores) \n",
    "    auc = roc_auc_score(Y, anomalyscores)\n",
    "    print(\"xstream: AP =\", ap, \"AUC =\", auc)\n",
    "\n",
    "anomalyscores = (anomalyscores - anomalyscores.min(axis=0)) / (anomalyscores.max(axis=0) - anomalyscores.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50131f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 289 prediction precision: 1.000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# find the top k\n",
    "if use_label is True:\n",
    "    if has_label is False:\n",
    "        print(\"Should provide actual labels, exit the program\")\n",
    "        exit()\n",
    "    else:\n",
    "        top_index = np.array( [idx for idx,i in enumerate(Y) if i == 1])\n",
    "else:\n",
    "    top_index = np.argsort(anomalyscores)[::-1][0:topk]\n",
    "X_explain = projected_X[top_index]\n",
    "if has_label:\n",
    "    result = np.concatenate((top_index.reshape(topk,1), anomalyscores[top_index].reshape(topk,1),\\\n",
    "                     Y.to_numpy()[top_index].reshape(topk,1)), axis = 1)\n",
    "    print(\"top %d prediction precision: %.3f\" %(topk,np.sum(result[:,2] == 1.0) / topk))\n",
    "    np.savetxt(output_path + \"/\" + \"anomaly_scores.txt\", result, delimiter = \",\")\n",
    "else:\n",
    "    result = np.concatenate((top_index.reshape(topk,1), anomalyscores[top_index].reshape(topk,1)),axis = 1)\n",
    "    #print(\"top %d prediction precision: %.3f\" %(topk,np.sum(result[:,2] == 1.0) / topk))\n",
    "    np.savetxt(output_path + \"/\" + \"anomaly_scores.txt\", result, delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee857ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the outliers\n",
    "if is_explain:\n",
    "    fimportance = score_in_chains(cf, X_explain)\n",
    "    if projection:\n",
    "        explain_object = Explanation(X,\n",
    "                         feature_names =feature_names,\n",
    "                         explanation_type = \"random_walk\", \n",
    "                         projdim = projdim)\n",
    "        ex = explain_object.explain(fimportance,result)\n",
    "        X_transform = np.array(ex)\n",
    "    else:\n",
    "        X_transform = np.array(fimportance)\n",
    "\n",
    "    # save the results with clustering indices\n",
    "    X_result = np.concatenate((result[:,0:1], X_transform),axis =1)\n",
    "    np.savetxt(output_path + \"/\" + \"explanations.txt\",X_result,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e4b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformed, data_MDS = get_mds(X_transform)\n",
    "max_k = parameters.cluster_num\n",
    "\n",
    "sil = []\n",
    "label_lst = []\n",
    "for i in range(1, max_k):\n",
    "    if i == 1:\n",
    "        labels= np.zeros((data_transformed.shape[0],1))\n",
    "        label_lst.append(labels)\n",
    "        col_name = str(i) + \" clusters\"\n",
    "        data_MDS[col_name] = labels\n",
    "    else:\n",
    "        kmeans = KMeans(n_clusters=i, random_state=0).fit(data_transformed)\n",
    "        labels = kmeans.labels_\n",
    "        label_lst.append(labels)\n",
    "        sil.append(silhouette_score(data_transformed, labels, metric=\"euclidean\"))\n",
    "        col_name = str(i) + \" clusters\"\n",
    "        data_MDS[col_name] = labels\n",
    "\n",
    "data_MDS = data_MDS.reset_index()\n",
    "data_MDS = data_MDS.drop(\"index\",axis=1)\n",
    "\n",
    "optimal_k = np.argmax(sil) + 2     \n",
    "cluster_data = data_MDS\n",
    "\n",
    "outlier = X.iloc[top_index]\n",
    "cluster_data.index = outlier.index\n",
    "data_explain = pd.DataFrame(X_transform, columns=[feat + \"_ex\" for feat in feature_names])\n",
    "data_explain.index = outlier.index\n",
    "\n",
    "optimal = np.array(optimal_k)\n",
    "optimal = np.repeat(optimal,topk)\n",
    "optimal_split = pd.DataFrame(optimal, columns = [\"optimal cluster\"])\n",
    "optimal_split.index = outlier.index\n",
    "if has_label:\n",
    "    anomaly_scores = pd.DataFrame(result[:,1:], columns =[ \"anomaly_scores\", \"label\"],index = result[:,0])\n",
    "else:\n",
    "    anomaly_scores = pd.DataFrame(result[:,1:], columns =[ \"anomaly_scores\"],index = result[:,0])\n",
    "final_result = pd.concat([cluster_data, optimal_split, data_explain, outlier, anomaly_scores],axis=1)\n",
    "final_result.index.name = \"index\"\n",
    "final_result.to_csv(output_path + \"/\" + \"concatenate_result.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f732ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_split.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff85f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_explain.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ed57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outlier.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c178cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anomaly_scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1600e834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
